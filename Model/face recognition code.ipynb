{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2a9afe",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1d1735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 encoding images found.\n",
      "Encoding images loaded\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "frame_resizing = 0.25\n",
    "\n",
    "def load_encoding_images(images_path):\n",
    "    \"\"\"\n",
    "    Load encoding images from path\n",
    "    :param images_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Load Images\n",
    "    images_path = glob.glob(os.path.join(images_path, \"*.*\"))\n",
    "\n",
    "    print(\"{} encoding images found.\".format(len(images_path)))\n",
    "\n",
    "    # Store image encoding and names\n",
    "    for img_path in images_path:\n",
    "        img = cv2.imread(img_path)\n",
    "        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get the filename only from the initial file path.\n",
    "        basename = os.path.basename(img_path)\n",
    "        (filename, ext) = os.path.splitext(basename)\n",
    "        # Get encoding\n",
    "        img_encoding = face_recognition.face_encodings(rgb_img)[0]\n",
    "\n",
    "        # Store file name and file encoding\n",
    "        known_face_encodings.append(img_encoding)\n",
    "        known_face_names.append(filename)\n",
    "    print(\"Encoding images loaded\")\n",
    "\n",
    "def detect_known_faces(frame):\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    # Find all the faces and face encodings in the current frame of video\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "    face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "    face_names = []\n",
    "    for face_encoding in face_encodings:\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # # If a match was found in known_face_encodings, just use the first one.\n",
    "        # if True in matches:\n",
    "        #     first_match_index = matches.index(True)\n",
    "        #     name = known_face_names[first_match_index]\n",
    "\n",
    "        # Or instead, use the known face with the smallest distance to the new face\n",
    "        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "        best_match_index = np.argmin(face_distances)\n",
    "        if matches[best_match_index]:\n",
    "            name = known_face_names[best_match_index]\n",
    "        face_names.append(name)\n",
    "\n",
    "    return face_names\n",
    "\n",
    "# Encode faces from a folder\n",
    "load_encoding_images(\"images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f65d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    print(frame,type(frame),frame.shape)\n",
    "    # Detect Faces\n",
    "    face_locations, face_names = detect_known_faces(frame)\n",
    "    for face_loc, name in zip(face_locations, face_names):\n",
    "        y1, x2, y2, x1 = face_loc[0], face_loc[1], face_loc[2], face_loc[3]\n",
    "\n",
    "        cv2.putText(frame, name,(x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 200), 2)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 200), 4)\n",
    "                \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570dda9",
   "metadata": {},
   "source": [
    "# Server code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c16ff714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [08/Jul/2023 16:38:03] \"OPTIONS /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:38:03] \"POST /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:38:06] \"POST /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:38:35] \"OPTIONS /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:38:35] \"POST /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:41:41] \"OPTIONS /test-image HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Jul/2023 16:41:42] \"POST /test-image HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# server code\n",
    "\n",
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "from flask import Flask , request \n",
    "from flask_cors import CORS\n",
    "from codecs import encode\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route(\"/test-image\",methods=[\"POST\"])\n",
    "def test_image():\n",
    "     #getting the base64 image string from the JSON in the request\n",
    "    image_b64 = request.json[0]['image']\n",
    "    parts = image_b64.split('base64,')\n",
    "    #getting the image data \n",
    "    data = parts[1]\n",
    "    \n",
    "    #decoding the image\n",
    "    bytes_img = encode(data, 'utf-8')\n",
    "    binary_img = base64.decodebytes(bytes_img)\n",
    "    image = Image.open(BytesIO(binary_img))\n",
    "    image_ndarray = np.array(image)\n",
    "    \n",
    "    # preforming the face recognition \n",
    "    face_names = detect_known_faces(image_ndarray)\n",
    "    \n",
    "    return {'message': 'Image scaned successfully.','username':str(face_names)}, 200\n",
    "\n",
    "\n",
    "@app.route('/save-image', methods=['POST'])\n",
    "def save_image():\n",
    "     #getting the base64 image string from the JSON in the request\n",
    "    image_b64 = request.json[0]['image']\n",
    "    name = request.json[0]['username']\n",
    "    parts = image_b64.split('base64,')\n",
    "    #getting the image data \n",
    "    data = parts[1]\n",
    "    \n",
    "    #decoding the image\n",
    "    bytes_img = encode(data, 'utf-8')\n",
    "    binary_img = base64.decodebytes(bytes_img)\n",
    "    image = Image.open(BytesIO(binary_img))\n",
    "    image_ndarray = np.array(image)\n",
    "\n",
    "    #saving the image\n",
    "    save_path = 'images\\\\test\\\\'+name+'.png'\n",
    "    with open(save_path, 'wb') as f:\n",
    "        f.write(binary_img)\n",
    "    return {'message': 'Image saved successfully.'}, 200\n",
    "\n",
    "if __name__=='__main__':\n",
    "   app.run(debug=False,port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13c9ec",
   "metadata": {},
   "source": [
    "# code 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04126816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Looking Right\n",
      "Looking Right\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Left\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Right\n",
      "Looking Right\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Up\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Looking Right\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n",
      "Forward\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13792/1773759653.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Head Pose Estimation'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;36m0xFF\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Flip the image horizontally for a later selfie-view display\n",
    "    # Also convert the color space from BGR to RGB\n",
    "    image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # To improve performance\n",
    "    image.flags.writeable = False\n",
    "    \n",
    "    # Get the result\n",
    "    results = face_mesh.process(image)\n",
    "    \n",
    "    # To improve performance\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Convert the color space from RGB to BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    face_3d = []\n",
    "    face_2d = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx == 33 or idx == 263 or idx == 1 or idx == 61 or idx == 291 or idx == 199:\n",
    "                    if idx == 1:\n",
    "                        nose_2d = (lm.x * img_w, lm.y * img_h)\n",
    "                        nose_3d = (lm.x * img_w, lm.y * img_h, lm.z * 3000)\n",
    "\n",
    "                    x, y = int(lm.x * img_w), int(lm.y * img_h)\n",
    "\n",
    "                    # Get the 2D Coordinates\n",
    "                    face_2d.append([x, y])\n",
    "\n",
    "                    # Get the 3D Coordinates\n",
    "                    face_3d.append([x, y, lm.z])       \n",
    "            \n",
    "            # Convert it to the NumPy array\n",
    "            face_2d = np.array(face_2d, dtype=np.float64)\n",
    "\n",
    "            # Convert it to the NumPy array\n",
    "            face_3d = np.array(face_3d, dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "            focal_length = 1 * img_w\n",
    "\n",
    "            cam_matrix = np.array([ [focal_length, 0, img_h / 2],\n",
    "                                    [0, focal_length, img_w / 2],\n",
    "                                    [0, 0, 1]])\n",
    "\n",
    "            # The distortion parameters\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Solve PnP\n",
    "            success, rot_vec, trans_vec = cv2.solvePnP(face_3d, face_2d, cam_matrix, dist_matrix)\n",
    "\n",
    "            # Get rotational matrix\n",
    "            rmat, jac = cv2.Rodrigues(rot_vec)\n",
    "\n",
    "            # Get angles\n",
    "            angles, mtxR, mtxQ, Qx, Qy, Qz = cv2.RQDecomp3x3(rmat)\n",
    "\n",
    "            # Get the y rotation degree\n",
    "            x = angles[0] * 360\n",
    "            y = angles[1] * 360\n",
    "            z = angles[2] * 360\n",
    "          \n",
    "\n",
    "            # See where the user's head tilting\n",
    "            if y < -10:\n",
    "               print( \"Looking Left\")\n",
    "            elif y > 10:\n",
    "                print( \"Looking Right\")\n",
    "            elif x < -10:\n",
    "                print( \"Looking Down\")\n",
    "            elif x > 10:\n",
    "                print( \"Looking Up\")\n",
    "            else:\n",
    "                print( \"Forward\")\n",
    "\n",
    "\n",
    "        end = time.time()\n",
    "        totalTime = end - start\n",
    "\n",
    "        fps = 1 / totalTime\n",
    "       \n",
    "    cv2.imshow('Head Pose Estimation', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e5d3e6",
   "metadata": {},
   "source": [
    "# code 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9310733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "# Load Face Detection Model\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "# Load Anti-Spoofing Model graph\n",
    "json_file = open('/antispoofing_model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load antispoofing model weights \n",
    "model.load_weights('antispoofing_model.h5')\n",
    "print(\"Model loaded from disk\")\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    try:\n",
    "        ret,frame = video.read()\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "        for (x,y,w,h) in faces:  \n",
    "            face = frame[y-5:y+h+5,x-5:x+w+5]\n",
    "            resized_face = cv2.resize(face,(160,160))\n",
    "            resized_face = resized_face.astype(\"float\") / 255.0\n",
    "            # resized_face = img_to_array(resized_face)\n",
    "            resized_face = np.expand_dims(resized_face, axis=0)\n",
    "            # pass the face ROI through the trained liveness detector\n",
    "            # model to determine if the face is \"real\" or \"fake\"\n",
    "            preds = model.predict(resized_face)[0]\n",
    "            #print(preds)\n",
    "            if preds> 0.5:\n",
    "                label = 'spoof'\n",
    "                print(label)\n",
    "                cv2.putText(frame, label, (x,y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w,y+h),\n",
    "                    (0, 0, 255), 2)\n",
    "            else:\n",
    "                label = 'real'\n",
    "                print(label)\n",
    "                cv2.putText(frame, label, (x,y - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w,y+h),\n",
    "                (0, 255, 0), 2)q\n",
    "        cv2.imshow('frame', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        pass\n",
    "video.release()        \n",
    "cv2.destroyAllWindows()import cv2\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "root_dir = os.getcwd()\n",
    "# Load Face Detection Model\n",
    "face_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "# Load Anti-Spoofing Model graph\n",
    "json_file = open('/antispoofing_model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "# load antispoofing model weights \n",
    "model.load_weights('antispoofing_model.h5')\n",
    "print(\"Model loaded from disk\")\n",
    "\n",
    "video = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    try:\n",
    "        ret,frame = video.read()\n",
    "        gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray,1.3,5)\n",
    "        for (x,y,w,h) in faces:  \n",
    "            face = frame[y-5:y+h+5,x-5:x+w+5]\n",
    "            resized_face = cv2.resize(face,(160,160))\n",
    "            resized_face = resized_face.astype(\"float\") / 255.0\n",
    "            # resized_face = img_to_array(resized_face)\n",
    "            resized_face = np.expand_dims(resized_face, axis=0)\n",
    "            # pass the face ROI through the trained liveness detector\n",
    "            # model to determine if the face is \"real\" or \"fake\"\n",
    "            preds = model.predict(resized_face)[0]\n",
    "            #print(preds)\n",
    "            if preds> 0.5:\n",
    "                label = 'spoof'\n",
    "                print(label)\n",
    "                cv2.putText(frame, label, (x,y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w,y+h),\n",
    "                    (0, 0, 255), 2)\n",
    "            else:\n",
    "                label = 'real'\n",
    "                print(label)\n",
    "                cv2.putText(frame, label, (x,y - 10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,0), 2)\n",
    "                cv2.rectangle(frame, (x, y), (x+w,y+h),\n",
    "                (0, 255, 0), 2)q\n",
    "        cv2.imshow('frame', frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "    except Exception as e:\n",
    "        pass\n",
    "video.release()        \n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
